Parsec/Lexer«import«Math/Arith¸ Lang/RegExp¸ Parsec/Input»¸ ---«"# Lexical Analyzer

This module implements a lexical analyzer a.k.a. _lexer_ for converting a string to a stream tokens. For complex grammars it's more efficient to preprocess a string to tokens before parsing it."»¸ type«---«"## Token Type

Token is a parameterized type whose first argument is the token type, typically an enumeration. The second argument is either a regexp used to recognize the token or the token itself as string."»¸ data«Token«s»¸ Token«s¸ String»»¸ ---«"## Lexer Type

Lexer type consists of array of tokens and the combined regexp that recognizes each of them as a separate group."»¸ data«Lexer«s»¸ Lexer«[«Token«s»»¸ RegExp»»»¸ define«---«"## Creating a Lexer

To create a lexer you need to specify the tokens for the `newLexer` function. It constructs the combined regexp that recognizes all the tokens."»¸ newLexer«->«[«Token«s»»¸ Lexer«s»»¸ ->«tokens¸ do«let«matcher¸ @join«@map«tokens¸ ->>«token¸ i¸ do«let«Token«_¸ re»¸ token»¸ _concat«[«"(?<g"¸ _show«i»¸ ">"¸ re¸ ")"»»»»»¸ "|"»»¸ Lexer«tokens¸ _reflags«matcher¸ "yu"»»»»»¸ ---«"## Match a Token with Lexer

The main task of a lexer is to return the next token from an input string. `matchToken` function does this by taking the lexer, input string, and input position as arguments. It returns a token at the position or `Null` if none was found."»¸ matchToken«->«Lexer«s»¸ String¸ Number¸ ?«Token«s»»»¸ ->«Lexer«tokens¸ matcher»¸ input¸ pos¸ do«RegExp:«.lastIndex.set«matcher¸ pos»»¸ match«RegExp:«.exec«matcher¸ input»»¸ |«?«m»¸ @first«tokens¸ ->>«token¸ i¸ match«:get«RegExpExecArray:«.groups«m»»¸ ##«"g"¸ _show«i»»»¸ |«?«s»¸ do«let«Token«t¸ _»¸ token»¸ ?«Token«t¸ s»»»»¸ |«_¸ Null»»»»»¸ |«_¸ Null»»»»»¸ ---«"## Lexer as Input

Now we can adapt the lexer to act as an input for the parser. Given an input string, a lexer, and the token representing end of input `eof`, `lexerInput` returns a `ParserInput` object that can be used for that.

The function builds a closure that remembers the position in the input string and the current token. If we are not at the end of input, it calls `matchToken` to get the next token and advance the position.


The closure also caches the tokens read so far in the `tokens` array. The token at input position `i` is stored in `tokens @ i`. Before calling `matchToken` we check if the `tokens` array already contains the token. This makes backtracking more efficient as we don't have to reprocess the input."»¸ lexerInput«->«String¸ Lexer«a»¸ Token«a»¸ ParserInput«Token«a»»»¸ ->«input¸ lexer¸ eof¸ do«set«pos¸ -1»¸ set«curr¸ eof»¸ let«tokens¸ [»¸ let«next¸ \«if«<«^«pos»¸ _strLen«input»»¸ do«+=«pos¸ match«@«tokens¸ ^«pos»»¸ |«Token«_¸ text»¸ _strLen«text»»¸ |«_¸ 1»»»¸ match«@«tokens¸ ^«pos»»¸ |«tok¸ tok»¸ |«_¸ match«matchToken«lexer¸ input¸ ^«pos»»¸ |«?«tok»¸ do«@set«tokens¸ ^«pos»¸ tok»¸ :=«curr¸ tok»»»¸ |«_¸ throw«"Invalid token"»»»»»»¸ :=«curr¸ eof»»»»¸ {«position«\«^«pos»»»¸ next«next»¸ current«\«^«curr»»»»»»»»»