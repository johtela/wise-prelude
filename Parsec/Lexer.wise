Parsec/Lexer«import«Math/Arith¸ Lang/RegExp¸ Parsec/Input»¸ "# Lexical Analyzer

This module implements a lexical analyzer a.k.a. _lexer_ for converting a string to a stream tokens. For complex grammars it's more efficient to preprocess a string to tokens before parsing it."¸ type«"## Token Type

Token is a parameterized type whose first argument is the token type, typically an enumeration. The second argument is either a regexp used to recognize the token or the token itself as string."¸ data«Token«s»¸ .Token«s¸ String»»¸ "## Lexer Type

Lexer type consists of array of tokens and the combined regexp that recognizes each of them as a separate group."¸ data«Lexer«s»¸ .Lexer«[«Token«s»»¸ RegExp»»»¸ instance«Show«Token«a»¸ _show«->>«token¸ do«let«.Token«_¸ str»¸ token»¸ str»»»»»¸ define«"## Creating a Lexer

To create a lexer you need to specify the tokens for the `newLexer` function. It constructs the combined regexp that recognizes all the tokens."¸ newLexer«->«[«Token«s»»¸ Lexer«s»»¸ ->«tokens¸ do«let«matcher¸ @join«@map«tokens¸ ->>«token¸ i¸ do«let«.Token«_¸ re»¸ token»¸ _concat«[«"(?<g"¸ _show«i»¸ ">"¸ re¸ ")"»»»»»¸ "|"»»¸ .Lexer«tokens¸ _reflags«matcher¸ "yu"»»»»»¸ "## Match a Token with Lexer

The main task of a lexer is to return the next token from an input string. `matchToken` function does this by taking the lexer, input string, and input position as arguments. It returns a token at the position or `Null` if none was found."¸ matchToken«->«Lexer«s»¸ String¸ Number¸ ?«Token«s»»»¸ ->«.Lexer«tokens¸ matcher»¸ input¸ pos¸ do«RegExp:«.lastIndex.set«matcher¸ pos»»¸ match«RegExp:«.exec«matcher¸ input»»¸ |«?«m»¸ @first«tokens¸ ->>«token¸ i¸ match«:get«RegExpExecArray:«.groups«m»»¸ ##«"g"¸ _show«i»»»¸ |«?«s»¸ do«let«.Token«t¸ _»¸ token»¸ ?«.Token«t¸ s»»»»¸ |«_¸ Null»»»»»¸ |«_¸ Null»»»»»¸ "## Lexer as Input

The lexer can now serve as input for the parser. The `lexerInput` function takes an input string, a lexer, and an end-of-input token (`eof`) to produce a `ParserInput` object.

This function creates a closure that tracks the current position in the input and the current token. If the end of input hasn’t been reached, it uses `matchToken` to fetch the next token and advance the position.

To optimize backtracking, the closure caches tokens in a `tokens` array. A token at position `i` is stored in `tokens & i`. Before calling `matchToken`, we check the cache to avoid reprocessing input."¸ lexerInput«->«String¸ Lexer«a»¸ Token«a»¸ ParserInput«Token«a»»»¸ ->«input¸ lexer¸ eof¸ do«set«pos¸ -1»¸ set«curr¸ eof»¸ let«tokens¸ [»¸ let«next¸ \«if«<«^«pos»¸ _strLen«input»»¸ do«+=«pos¸ match«@«tokens¸ ^«pos»»¸ |«.Token«_¸ text»¸ _strLen«text»»¸ |«_¸ 1»»»¸ match«@«tokens¸ ^«pos»»¸ |«tok¸ tok»¸ |«_¸ match«matchToken«lexer¸ input¸ ^«pos»»¸ |«?«tok»¸ do«@set«tokens¸ ^«pos»¸ tok»¸ :=«curr¸ tok»»»¸ |«_¸ throw«"Invalid token"»»»»»»¸ :=«curr¸ eof»»»»¸ {«position«\«^«pos»»»¸ setPosition«->«p¸ :=«pos¸ p»»»¸ next«next»¸ current«\«^«curr»»»»»»»»»